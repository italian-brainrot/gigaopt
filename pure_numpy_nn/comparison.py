import numpy as np
import optuna

from .neural_net import NeuralNetwork, mse_loss, mse_loss_derivative
from .optimizers import Adam, AdaThird, Nova
from .dataset import generate_data, get_mini_batches

# Hyperparameters
N_SAMPLES = 1000
N_FEATURES = 32
# 5 layers: 1 input, 3 hidden, 1 output
LAYER_SIZES = [N_FEATURES, 64, 128, 64, N_FEATURES]
EPOCHS = 101 # Run for 100 epochs, print every 10
BATCH_SIZE = 32

def run_experiment(optimizer_class, optimizer_params, n_samples, n_features, layer_sizes, epochs, batch_size):
    """
    Runs a training experiment for a given optimizer, with learning rate tuning.
    """
    # For reproducibility
    np.random.seed(42)

    # Generate data
    X, y = generate_data(n_samples, n_features)

    def objective(trial):
        # Suggest learning rate
        lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)

        # Create a copy of params and update lr
        current_params = optimizer_params.copy()
        current_params['lr'] = lr

        # Initialize network and optimizer
        net = NeuralNetwork(layer_sizes)
        optimizer = optimizer_class(**current_params)

        # Training loop
        for epoch in range(epochs):
            epoch_loss = 0
            num_batches = 0
            for x_batch, y_batch in get_mini_batches(X, y, batch_size):
                y_pred = net.forward(x_batch)
                loss = mse_loss(y_batch, y_pred)
                epoch_loss += loss
                num_batches += 1
                loss_grad = mse_loss_derivative(y_batch, y_pred)
                grads = net.backward(loss_grad)
                params = net.get_params()
                optimizer.step(params, grads)

            avg_loss = epoch_loss / num_batches
            trial.report(avg_loss, epoch)

            if trial.should_prune():
                raise optuna.exceptions.TrialPruned()

        return avg_loss

    study = optuna.create_study(direction="minimize", pruner=optuna.pruners.HyperbandPruner())
    study.optimize(objective, n_trials=100)

    best_lr = study.best_trial.params['lr']
    print(f"Best LR for {optimizer_class.__name__}: {best_lr}")

    # Now, run the experiment with the best learning rate to get the losses
    np.random.seed(42)
    net = NeuralNetwork(layer_sizes)
    best_params = optimizer_params.copy()
    best_params['lr'] = best_lr
    optimizer = optimizer_class(**best_params)

    losses = []
    for epoch in range(epochs):
        epoch_loss = 0
        num_batches = 0
        for x_batch, y_batch in get_mini_batches(X, y, batch_size):
            y_pred = net.forward(x_batch)
            loss = mse_loss(y_batch, y_pred)
            epoch_loss += loss
            num_batches += 1
            loss_grad = mse_loss_derivative(y_batch, y_pred)
            grads = net.backward(loss_grad)
            params = net.get_params()
            optimizer.step(params, grads)

        avg_loss = epoch_loss / num_batches
        losses.append(avg_loss)

    return losses

def main():
    """
    Main comparison function.
    After an experiment is ran, it should be commented out and replaced with losses for efficiency.
    """
    # # Adam experiment
    # adam_params = {}
    # adam_losses = run_experiment(Adam, adam_params, N_SAMPLES, N_FEATURES, LAYER_SIZES, EPOCHS, BATCH_SIZE)
    adam_losses = [0.5625357682330576, 0.5311177600406631, 0.5216330331620692, 0.5097249861737797, 0.4954322245402534, 0.481145406996237, 0.46833225571677684, 0.4522401357942043, 0.43950964942222875, 0.42815907567585904, 0.4115257779928213, 0.40143712224412054, 0.3904900555042921, 0.3785659741586357, 0.3673851267759969, 0.3600220633560651, 0.35145367859905785, 0.3442436470656205, 0.3356841043046049, 0.32847155962697483, 0.3231180686968785, 0.3152283682626572, 0.3088175156401371, 0.30395967627705434, 0.2999184765268569, 0.2946691026693682, 0.2892319200489967, 0.28401098525620194, 0.2809786431821191, 0.27799133853206387, 0.27301684191744985, 0.26876912327318314, 0.2676889190229899, 0.263666068464845, 0.26170125443592807, 0.25920983675138354, 0.2540818176005698, 0.25239878477659283, 0.24978648072188164, 0.24814863595341938, 0.2450159657865079, 0.2420475902071018, 0.23835203133209762, 0.23804739008001005, 0.23607075094305527, 0.23527757051701956, 0.23085143029842642, 0.22964911186753784, 0.22728943142510757, 0.22514562973600954, 0.2253590151690248, 0.2220086779999298, 0.22053169714394397, 0.21969235718364166, 0.21827040459285268, 0.21480527307784114, 0.21459855118882495, 0.21255021298704543, 0.21337247108546264, 0.21182384692830572, 0.20967758116382645, 0.20738709271838682, 0.20684511640010544, 0.20688765500037698, 0.2054677541269584, 0.20394348301102017, 0.20528989243746937, 0.20457940163109387, 0.20281888743937285, 0.20143706129697006, 0.2009392327155502, 0.19852553744901036, 0.19675574628995535, 0.1973197056658818, 0.1976916722573181, 0.19749075383581283, 0.19554914664511583, 0.19489013974981165, 0.19463071924159253, 0.1922864353731607, 0.19083914113962053, 0.19068500875170125, 0.18999474046906073, 0.18948453553331515, 0.18773531514426128, 0.1872156684513949, 0.18615666427743824, 0.18715338385925248, 0.18803726095841938, 0.1869596265251795, 0.18534088572127078, 0.1843258858479626, 0.18386434638817628, 0.18161559587512938, 0.18340159462224728, 0.18104408804282385, 0.17930438732913095, 0.18082148480142796, 0.18157741962602844, 0.18177050584842455, 0.17723168193770694]

    # # AdaThird experiment
    # adathird_params = {'beta3': 0.99}
    # adathird_losses = run_experiment(AdaThird, adathird_params, N_SAMPLES, N_FEATURES, LAYER_SIZES, EPOCHS, BATCH_SIZE)
    adathird_losses = [0.5603100096773633, 0.5314254962498074, 0.5221762490393964, 0.5099806601900624, 0.49389676588837184, 0.47930895490124875, 0.4659562151909206, 0.4498399392960178, 0.4352387270929744, 0.4248191799078883, 0.4096850915420343, 0.39844631997906377, 0.3880055970572228, 0.37428940052417187, 0.36562553760506183, 0.35863981308889115, 0.3490636778943015, 0.3423111652689572, 0.33373319063007906, 0.32664211092293427, 0.3218917174141892, 0.31309768868988963, 0.3077612258088446, 0.30205637123525275, 0.296838452307631, 0.29184286056008646, 0.2884387032320576, 0.2833562039515732, 0.279366747858588, 0.2790995883196017, 0.27450984290652214, 0.2697818463306668, 0.2683567081280264, 0.2627417084239992, 0.25852107082421943, 0.25628565773937334, 0.2536376435022436, 0.24893998891118596, 0.2477319609052388, 0.24710456405443573, 0.24453778385382946, 0.24285117485944702, 0.24099920986738246, 0.23647723827772016, 0.23513364364101494, 0.23362998803742507, 0.2312198010818826, 0.2307665726193836, 0.2268838525172077, 0.22473585136539892, 0.22412721917609518, 0.2202562844898446, 0.22207096778868077, 0.21972422522408977, 0.21819696314649528, 0.21576882926254928, 0.21612913945259757, 0.21700864215274912, 0.21516893900268733, 0.2126868055586226, 0.21114709330857287, 0.20982083691177866, 0.20794635510552759, 0.20527677480021544, 0.20561935955671543, 0.20428304345911216, 0.20539684859253216, 0.20497603158239688, 0.20325229101126402, 0.20057647834958278, 0.20091858525505502, 0.1996629763395368, 0.1984026281442465, 0.1975204036505938, 0.19709163978538058, 0.19564913659451094, 0.19388095893065638, 0.19373362385402598, 0.19305370493630067, 0.1927157071119821, 0.19173313160883487, 0.19078458151887698, 0.19032564645242922, 0.1899659105994468, 0.18836999704704332, 0.18941875504963643, 0.18882245398707803, 0.18650447548792362, 0.18474806153190826, 0.18420975391884964, 0.1849743115772937, 0.18316322503901664, 0.18194775330671997, 0.183064861588224, 0.18210953070411692, 0.18181617513366122, 0.18150864104660155, 0.18129990531873424, 0.17956769501440012, 0.17968835400807084, 0.1769168308425637]

    # # Nova experiment
    # nova_params = {}
    # nova_losses = run_experiment(Nova, nova_params, N_SAMPLES, N_FEATURES, LAYER_SIZES, EPOCHS, BATCH_SIZE)
    nova_losses = [0.5558134304890057, 0.5310694878566131, 0.5226641286208271, 0.5088400866543983, 0.49437060144972217, 0.4800179959995498, 0.4649562924309176, 0.4485021512771691, 0.4332321497836661, 0.4190402823324727, 0.40266740259945294, 0.39152727387426334, 0.37861971817993223, 0.36894698134742754, 0.3575820922510853, 0.3504858185015601, 0.3388700035063466, 0.33139167417195275, 0.3240750434738246, 0.31792117390517866, 0.31317508809018807, 0.303878895643928, 0.30122831910576064, 0.29612271685964253, 0.28911267553832753, 0.28744949094225913, 0.2834863493255326, 0.2753308481176764, 0.2734009925978378, 0.2715372520657596, 0.26529118373802346, 0.2598508960806173, 0.258924794922688, 0.2555454966804539, 0.2523521864641628, 0.25032936585113597, 0.24786022937393098, 0.24609788963465, 0.24594373012315301, 0.24271387980481798, 0.23803497226018114, 0.23973553559003455, 0.2353284399943369, 0.2364166843777477, 0.23622149079353286, 0.23399236355759806, 0.23316207238617875, 0.23101775407892422, 0.22602928493385666, 0.22554439316114497, 0.22638864240973494, 0.2228102757866563, 0.22178813343491005, 0.22015177997551075, 0.22045296816069251, 0.21896573251026002, 0.2194197956404946, 0.21317307085704898, 0.21343796413852614, 0.21282872694499183, 0.21249234121157243, 0.21049677755983223, 0.20717434050154374, 0.2085937479670714, 0.21006027320714799, 0.20832849277051388, 0.206756905246061, 0.20890607290995342, 0.20713067404703436, 0.2054597466436064, 0.2037052414656939, 0.20108023985716034, 0.20214424969573666, 0.20034548795674553, 0.19836380005168028, 0.1988058839500157, 0.19668293287799088, 0.19605918319806281, 0.19705408998741852, 0.1963065860849123, 0.19294084835385053, 0.1915343667822837, 0.1927401444312244, 0.19264259521446683, 0.19203427308615983, 0.19109842393302764, 0.19205880530802544, 0.1910640495740565, 0.19098292819740473, 0.19091161245322272, 0.18832646635744224, 0.18814378007010515, 0.18568156183305204, 0.1866933810450866, 0.18999123203343762, 0.18822669423011246, 0.1883456436487111, 0.18748741651764902, 0.18602254738082685, 0.1875830966492168, 0.18443070685599255]

    # Print comparison table
    print("Optimizer Performance Comparison")
    print("-" * 70)
    print(f"{'Epoch':<10}{'Adam Loss':<20}{'AdaThird Loss':<20}{'Nova Loss':<20}")
    print("-" * 70)
    for epoch in range(0, EPOCHS, 10):
        print(f"{epoch:<10}{adam_losses[epoch]:<20.4f}{adathird_losses[epoch]:<20.4f}{nova_losses[epoch]:<20.4f}")
    print("-" * 70)
    print(f"{'Final Loss':<10}{adam_losses[-1]:<20.4f}{adathird_losses[-1]:<20.4f}{nova_losses[-1]:<20.4f}")
    print("-" * 70)

if __name__ == "__main__":
    main()
